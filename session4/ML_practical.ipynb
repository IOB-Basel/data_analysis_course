{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through an example of a classification task.\n",
    "\n",
    "**Goals**:\n",
    "- set up a binary classifer using scikit-learn\n",
    "- classify cells from the rd10 dataset as either healthy (WT genotype) or diseased (rd10 homozygous genotype)\n",
    "- evaluate the model's performance using a test dataset\n",
    "- choose a penalty (options: None, \"l1\", \"l2\" or \"elasticnet\") and a suitable solver\n",
    "\n",
    "\n",
    "Scikit-learn documentation:\n",
    "- [Logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load dataset\n",
    "\n",
    "We will use the same dataset from the previous tutorial\n",
    "- Download dataset from the course google drive folder: `IOB (General) > PhD Program > Practical courses Spring 2024 > Data Analysis course June 2024 > session3 > practical > adata_course_filt.h5ad`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the path to the anndata object\n",
    "datadir = Path(\"\")\n",
    "\n",
    "adata = sc.read_h5ad(Path(datadir, \"adata_course_filt.h5ad\"))\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the labels that we are going to use in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[\"genotype\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Filter cells and genes using scanpy functions: `sc.pp.filter_cells` and `sc.pp.filter_genes`\n",
    "sc.pp.filter_cells()\n",
    "sc.pp.filter_genes()\n",
    "\n",
    "# Remove multiplets, projecting_neurons and undefined cells\n",
    "mask = adata.obs[\"cell_type\"].str.contains(\"mult|undefined|neuron\")\n",
    "adata = adata[~mask].copy()\n",
    "\n",
    "# Let's use highly_variable_genes to rank genes\n",
    "sc.pp.highly_variable_genes(adata, n_top_genes=10000, flavor=\"seurat_v3\")\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Baseline model\n",
    "\n",
    "**Task**: Use the pre-selected features (i.e., highly-variable genes) to train a classifier and report the accuracy of the predictions on the test set\n",
    "\n",
    "Note that the number of genes is greater than the number of cells. Let's start by selecting the top 200 highly variable genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_hv = adata[:, adata.var[\"highly_variable_rank\"] < 200]\n",
    "features = adata_hv.X\n",
    "target = adata_hv.obs[\"genotype\"]\n",
    "features.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Trick to get metadata associated with partitions\n",
    "# df_train, df_test, y_train_, y_test_ = train_test_split(adata_hv.obs, target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the class labels are well-balance in both the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression model\n",
    "logreg = LogisticRegression(penalty=None, max_iter=1000)\n",
    "\n",
    "# Train the logistic regression model\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy of logistic regression classifier on training set: {logreg.score(X_train, y_train):.4f}\")\n",
    "print(f\"Accuracy of logistic regression classifier on test set: {logreg.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Which of the two labels (`hom` or `wt`) can we predict more accurately?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Perform cross-validation to select regularization strength\n",
    "\n",
    "Let's now use a larger set of genes. To achieve this, we suggest applying a penalty to constrain the weights/parameters of the model and avoid overfitting.\n",
    "**Tip:**  Using all genes that pass QC steps would be ideal, but it can take too long to train the model. Therefore, consider a number of genes larger than 200 but fewer than 5000 (depending on your patience!).\n",
    "\n",
    "We will use grid search to find a suitable regularization strength (`C` as implemented in scikit-learn). Grid search is a fundamental strategy for tuning hyperparameters. The process involves the following steps. For each hyperparameter, define a set of candidate values. Then, train separate models using every possible combination of these values. Finally, select the configuration that yields the lowest validation error.\n",
    "\n",
    "\n",
    "**Tip:** Select a suitable solver depending on your choice of regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: subset genes using highly_variable_genes rank\n",
    "adata_hv = adata\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(adata_hv.X, adata_hv.obs[\"genotype\"], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the logistic regression model. Tips: choose `penalty`, `solver`, `max_iter`\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# TODO: Define the hyperparameter grid for regularization strength (C)\n",
    "param_grid = {\"C\": []}\n",
    "\n",
    "# Perform cross-validation to find the best regularization strength\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and the best regularization parameter\n",
    "best_logreg = grid_search.best_estimator_\n",
    "best_C = grid_search.best_params_[\"C\"]\n",
    "\n",
    "print(f\"Best regularization strength (C): {best_C}\")\n",
    "\n",
    "# Train the logistic regression model with the best parameter on the entire training set\n",
    "best_logreg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Accuracy of training set: {best_logreg.score(X_train, y_train):.4f}\")\n",
    "print(f\"Accuracy on test set: {best_logreg.score(X_test, y_test):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "results = grid_search.cv_results_\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(param_grid[\"C\"], results[\"mean_test_score\"], marker=\"o\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Regularization strength (C)\")\n",
    "plt.ylabel(\"Mean CV Accuracy\")\n",
    "plt.title(\"Cross-Validation accuracy for different regularization strengths\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Neuron networks\n",
    "\n",
    "Let's explore if using neural networks helps improving our classification task. Feel free to add as explore with the number of hidden layers and sizes, as well as the regularization strength (`alpha`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver=\"adam\", alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session4_practical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
