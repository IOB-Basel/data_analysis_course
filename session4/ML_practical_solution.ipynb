{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through an example of a classification task.\n",
    "\n",
    "**Goals**:\n",
    "- set up a binary classifer using scikit-learn\n",
    "- classify cells from the rd10 dataset as either healthy (WT genotype) or diseased (rd10 homozygous genotype)\n",
    "- evaluate the model's performance using a test dataset\n",
    "- choose a penalty (options: None, \"l1\", \"l2\" or \"elasticnet\") and a suitable solver\n",
    "\n",
    "\n",
    "Scikit-learn documentation:\n",
    "- [Logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load dataset\n",
    "\n",
    "\n",
    "We will use the same dataset from the previous tutorial\n",
    "- Download dataset from the course google drive folder: `IOB (General) > PhD Program > PC Spring 2025 > Data Analysis course June 2025 > session3 > practical > adata_course_filt.h5ad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Assumes that the anndata object is stored in a directory named \"data\" located one\n",
    "#       level up from the current directory\n",
    "datadir = Path(\"..\", \"data\")\n",
    "\n",
    "adata = sc.read_h5ad(Path(datadir, \"adata_course_filt.h5ad\"))\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the labels that we are going to use in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[\"genotype\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC gene and cell filters\n",
    "sc.pp.filter_cells(adata, min_genes=200)\n",
    "sc.pp.filter_genes(adata, min_cells=3)\n",
    "\n",
    "# Remove multiplets, projecting_neurons and undefined cells\n",
    "mask = adata.obs[\"cell_type\"].str.contains(\"mult|undefined|neuron\")\n",
    "adata = adata[~mask].copy()\n",
    "\n",
    "# Let's use highly_variable_genes to rank genes\n",
    "sc.pp.highly_variable_genes(adata, n_top_genes=10000, flavor=\"seurat_v3\")\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Baseline model\n",
    "\n",
    "**Task**: Use the pre-selected features (i.e., highly-variable genes) to train a classifier and report the accuracy of the predictions on the test set\n",
    "\n",
    "\n",
    "Note that the number of genes is greater than the number of cells. Let's start by selecting the top 200 highly variable genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_hv = adata[:, adata.var[\"highly_variable_rank\"] < 200]\n",
    "features = adata_hv.X\n",
    "target = adata_hv.obs[\"genotype\"]\n",
    "features.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Trick to get metadata associated with partitions\n",
    "# df_train, df_test, y_train_, y_test_ = train_test_split(adata_hv.obs, target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the class labels are well-balance in both the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression model\n",
    "logreg = LogisticRegression(penalty=None, max_iter=1000)\n",
    "\n",
    "# Train the logistic regression model\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy of logistic regression classifier on training set: {logreg.score(X_train, y_train):.4f}\")\n",
    "print(f\"Accuracy of logistic regression classifier on test set: {logreg.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Which of the two labels (`hom` or `wt`) can we predict more accurately?\n",
    "\n",
    "The `wt` label is predicted more accurately overall due to its higher recall and F1-score, despite the slightly lower precision compared to the `hom` label.\n",
    "In this context, precision refers to the proportion of correctly labeled cells among the predicted labels for each class, while recall denotes the proportion of correctly labeled cells among the actual labels for each class. Let's take a look at the confusion matrix: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, out of 1979 actual `hom` cells, we correctly identified 1500, yielding a recall of 0.76. Conversely, out of 1803 cells predicted as `hom`, our correct identification of 1500 gives a precision of 0.83.\n",
    "\n",
    "\n",
    "|            | pred. hom | pred. wt |\n",
    "|------------|-----------|----------|\n",
    "| actual hom | 1500      | 479      |\n",
    "| acutal wt  | 303       | 2022     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature standardization\n",
    "\n",
    "Scaling features is generally recommended in machine learning, though not always required. \n",
    "Algorithms that rely on gradient descent optimization, such as neural networks, support vector machines, and some linear models (like logistic regression), benefit from scaled features as they can converge faster. Also, algorithms relying on distance measures (such as k-nearest neighbors, k-means) are sensitive to the scale of input features, so it is important to apply feature scaling. Furthermore, when regularization is applied in the loss function, feature scaling ensures coefficients are penalized appropriately.\n",
    "\n",
    "On the other hand, Tree-based models (such as decision trees, random forests) are generally invariant to the scale of features. Scaling typically does not impact their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled_train = scaler.fit_transform(np.asarray(X_train.todense()))\n",
    "X_scaled_test = scaler.transform(np.asarray(X_test.todense()))\n",
    "\n",
    "# NOTE: the number of max. iterations is smaller\n",
    "logreg_scaled = LogisticRegression(penalty=None, max_iter=700)\n",
    "logreg_scaled.fit(X_scaled_train, y_train)\n",
    "\n",
    "print(f\"Accuracy of logistic regression classifier on training set: {logreg_scaled.score(X_scaled_train, y_train):.4f}\")\n",
    "print(f\"Accuracy of logistic regression classifier on test set: {logreg_scaled.score(X_scaled_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.where(np.abs(logreg_scaled.coef_[0]) > 15)[0]\n",
    "plt.scatter(\n",
    "    logreg.coef_[0],\n",
    "    logreg_scaled.coef_[0],\n",
    "    c=adata_hv.var[\"highly_variable_rank\"],\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "for idx in idxs:\n",
    "    plt.annotate(\n",
    "        adata_hv[:, idx].var_names[0],\n",
    "        (logreg.coef_[0, idx], logreg_scaled.coef_[0, idx]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0,10),\n",
    "        ha=\"center\"\n",
    "    )\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Unstandardized coefficients\")\n",
    "plt.ylabel(\"Standardized coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Perform cross-validation to select regularization strength\n",
    "\n",
    "Let's now use a larger set of genes. To achieve this, we suggest applying a penalty to constrain the weights/parameters of the model and avoid overfitting.\n",
    "**Tip:**  Using all genes that pass QC steps would be ideal, but it can take too long to train the model. Therefore, consider a number of genes larger than 200 but fewer than 5000 (depending on your patience!).\n",
    "\n",
    "We will use grid search to find a suitable regularization strength (`C` as implemented in scikit-learn). Grid search is a fundamental strategy for tuning hyperparameters. The process involves the following steps. For each hyperparameter, define a set of candidate values. Then, train separate models using every possible combination of these values. Finally, select the configuration that yields the lowest validation error.\n",
    "\n",
    "\n",
    "**Tip:** Select a suitable solver depending on your choice of regularizer\n",
    "\n",
    "Scikit-learn documentation:\n",
    "- [Tuning hyper-parameters](https://scikit-learn.org/stable/modules/grid_search.html#tuning-the-hyper-parameters-of-an-estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gene subset: extracting the top 1000 highly variable genes.\n",
    "adata_hv = adata[:, adata.var[\"highly_variable_rank\"] < 1000].copy()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(adata_hv.X, adata_hv.obs[\"genotype\"], test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled_train = scaler.fit_transform(np.asarray(X_train.todense()))\n",
    "X_scaled_test = scaler.transform(np.asarray(X_test.todense()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression model\n",
    "logreg_l2 = LogisticRegression(penalty=\"l2\", max_iter=500)\n",
    "\n",
    "# Define the hyperparameter grid for regularization strength (C)\n",
    "param_grid = {\"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "# Perform cross-validation to find the best regularization strength\n",
    "outfile = Path(datadir, \"grid_search_l2.pkl\")\n",
    "if outfile.exists():\n",
    "    with open(outfile, \"rb\") as f:\n",
    "        grid_search_l2 = pickle.load(f)\n",
    "else:\n",
    "    grid_search_l2 = GridSearchCV(logreg_l2, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "    grid_search_l2.fit(X_scaled_train, y_train)\n",
    "    with open(outfile, \"wb\") as f:\n",
    "        pickle.dump(grid_search_l2, f)\n",
    "\n",
    "\n",
    "# Get the best model and the best regularization parameter\n",
    "best_logreg_l2 = grid_search_l2.best_estimator_\n",
    "best_C = grid_search_l2.best_params_[\"C\"]\n",
    "\n",
    "print(f\"Best regularization strength (C): {best_C}\")\n",
    "\n",
    "# Train the logistic regression model with the best parameter on the entire training set\n",
    "best_logreg_l2.fit(X_scaled_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_logreg_l2.predict(X_scaled_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Accuracy of training set: {best_logreg_l2.score(X_scaled_train, y_train):.4f}\")\n",
    "print(f\"Accuracy on test set: {best_logreg_l2.score(X_scaled_test, y_test):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "results = grid_search_l2.cv_results_\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(param_grid[\"C\"], results[\"mean_test_score\"], marker=\"o\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Regularization strength (C)\")\n",
    "plt.ylabel(\"Mean CV Accuracy\")\n",
    "plt.title(\"Cross-Validation accuracy for different regularization strengths\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_hv.var[\"coef\"] = best_logreg_l2.coef_[0]\n",
    "\n",
    "mask = adata_hv.var[\"coef\"].abs() > 0.5\n",
    "plt.scatter(\n",
    "    adata_hv.var[\"highly_variable_rank\"],\n",
    "    adata_hv.var[\"coef\"],\n",
    ")\n",
    "for idx, row in adata_hv[:, mask].var.iterrows():\n",
    "    plt.annotate(\n",
    "        idx,\n",
    "        (row[\"highly_variable_rank\"], row[\"coef\"]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0,10),\n",
    "        ha=\"center\",\n",
    "    )\n",
    "plt.xlabel(\"Gene rank\")\n",
    "plt.ylabel(\"Standardized coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, we are fitting the probability of being a wild-type cell (`wt`) given its transcriptome. The coefficient for each feature can be exponentiated to get the odds ratio. If the coefficient of a given gene is negative, the odds ratio will be less than 1, indicating a decrease in the odds of the `wt` class as that gene is more expressed. Equivalently,since the logistic function maps the log-odds to probabilities, a negative coefficient implies that as the gene is more expressed, the probability of  being a wild-type cell decreases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 regularization\n",
    "\n",
    "Some advantages of the \"saga\" solver:\n",
    "- Uses a stochastic average gradient descent algorithm\n",
    "- Supports both L1 and L2 regularization, as well as elastic net regularization (a mix of L1 and L2).\n",
    "- Suitable for large datasets and high-dimensional data.\n",
    "- Speed: Efficient for large datasets, especially those with sparse features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression model\n",
    "logreg_l1 = LogisticRegression(solver=\"saga\", penalty=\"l1\", max_iter=5000)\n",
    "\n",
    "# Define the hyperparameter grid for regularization strength (C)\n",
    "param_grid = {\"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "# Perform cross-validation to find the best regularization strength\n",
    "outfile = Path(datadir, \"grid_search_l1.pkl\")\n",
    "if outfile.exists():\n",
    "    with open(outfile, \"rb\") as f:\n",
    "        grid_search_l1 = pickle.load(f)\n",
    "else:\n",
    "    grid_search_l1 = GridSearchCV(logreg_l1, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "    grid_search_l1.fit(X_scaled_train, y_train)\n",
    "    with open(outfile, \"wb\") as f:\n",
    "        pickle.dump(grid_search_l1, f)\n",
    "\n",
    "# Get the best model and the best regularization parameter\n",
    "best_logreg_l1 = grid_search_l1.best_estimator_\n",
    "best_C = grid_search_l1.best_params_[\"C\"]\n",
    "\n",
    "print(f\"Best regularization strength (C): {best_C}\")\n",
    "\n",
    "# Train the logistic regression model with the best parameter on the entire training set\n",
    "best_logreg_l1.fit(X_scaled_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_logreg_l1.predict(X_scaled_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Accuracy of training set: {best_logreg_l1.score(X_scaled_train, y_train):.4f}\")\n",
    "print(f\"Accuracy on test set: {best_logreg_l1.score(X_scaled_test, y_test):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "results = grid_search_l1.cv_results_\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(param_grid[\"C\"], results[\"mean_test_score\"], marker=\"o\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Regularization strength (C)\")\n",
    "plt.ylabel(\"Mean CV Accuracy\")\n",
    "plt.title(\"Cross-Validation accuracy for different regularization strengths\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of non-zero coefficients\n",
    "(best_logreg_l1.coef_ != 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_hv.var[\"coef\"] = best_logreg_l1.coef_[0]\n",
    "\n",
    "mask = adata_hv.var[\"coef\"].abs() > 0.7\n",
    "plt.scatter(\n",
    "    adata_hv.var[\"highly_variable_rank\"],\n",
    "    adata_hv.var[\"coef\"],\n",
    ")\n",
    "for idx, row in adata_hv[:, mask].var.iterrows():\n",
    "    plt.annotate(\n",
    "        idx,\n",
    "        (row[\"highly_variable_rank\"], row[\"coef\"]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0,10),\n",
    "        ha=\"center\",\n",
    "    )\n",
    "plt.xlabel(\"Gene rank\")\n",
    "plt.ylabel(\"Standardized coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = Path(datadir, \"grid_search_l1_full_model.pkl\")\n",
    "if outfile.exists():\n",
    "    with open(outfile, \"rb\") as f:\n",
    "        full_models = pickle.load(f)\n",
    "else:\n",
    "    # Extract non-zero parameters for each search\n",
    "    full_models = {}\n",
    "    for i, params in enumerate(grid_search_l1.cv_results_[\"params\"]):\n",
    "        model = copy.deepcopy(grid_search_l1.estimator)\n",
    "        full_models[i] = model.set_params(**params).fit(X_scaled_train, y_train)\n",
    "    with open(outfile, \"wb\") as f:\n",
    "        pickle.dump(full_models, f)\n",
    "\n",
    "for i, params in enumerate(grid_search_l1.cv_results_[\"params\"]):\n",
    "    non_zero_params = (full_models[i].coef_ != 0).sum()\n",
    "    print(f\"Search {i + 1} with params {params}: {non_zero_params} non-zero parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Lasso regression can produce non-unique solutions, especially in cases where the data exhibits multicollinearity or when the number of features exceeds the number of observations. This non-uniqueness means that different runs can yield different sets of non-zero coefficients. Consequently, it is not surprising that the number of non-zero parameters differs between `best_logreg_l1` and the results above, as Lasso's selection of features can vary depending on the specific conditions of each run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression model\n",
    "logreg = LogisticRegression(solver=\"saga\", penalty=\"elasticnet\", max_iter=3500)\n",
    "\n",
    "# Define the parameter grid for C and l1_ratio\n",
    "param_grid = {\n",
    "    \"C\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"l1_ratio\": [0.0, 0.25, 0.5, 0.75, 1.0],\n",
    "}\n",
    "\n",
    "# Perform cross-validation to find the best parameters\n",
    "outfile = Path(datadir, \"grid_search_elasticnet.pkl\")\n",
    "if outfile.exists():\n",
    "    with open(outfile, \"rb\") as f:\n",
    "        grid_search = pickle.load(f)\n",
    "else:\n",
    "    grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring=\"accuracy\", verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_scaled_train, y_train)\n",
    "    with open(outfile, \"wb\") as f:\n",
    "        pickle.dump(grid_search, f)\n",
    "\n",
    "\n",
    "# Get the best model and the best parameters\n",
    "best_logreg = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Train the logistic regression model with the best parameters on the entire training set\n",
    "best_logreg.fit(X_scaled_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_logreg.predict(X_scaled_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Accuracy of training set: {best_logreg.score(X_scaled_train, y_train):.4f}\")\n",
    "print(f\"Accuracy on test set: {best_logreg.score(X_scaled_test, y_test):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of non-zero coefficients\n",
    "(best_logreg.coef_ != 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_hv.var[\"coef\"] = best_logreg.coef_[0]\n",
    "\n",
    "mask = adata_hv.var[\"coef\"].abs() > 0.7\n",
    "plt.scatter(\n",
    "    adata_hv.var[\"highly_variable_rank\"],\n",
    "    adata_hv.var[\"coef\"],\n",
    ")\n",
    "for idx, row in adata_hv[:, mask].var.iterrows():\n",
    "    plt.annotate(\n",
    "        idx,\n",
    "        (row[\"highly_variable_rank\"], row[\"coef\"]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0,10),\n",
    "        ha=\"center\",\n",
    "    )\n",
    "plt.xlabel(\"Gene rank\")\n",
    "plt.ylabel(\"Standardized coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best solution was found with `l1_ratio=1`, which is equivalent to using an L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Neural networks\n",
    "\n",
    "Let's explore if using neural networks helps improving our classification task. Feel free to explore with the number of hidden layers and sizes, as well as the regularization strength (`alpha`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver=\"adam\", alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=42, max_iter=500)\n",
    "clf.fit(X_scaled_train, y_train)\n",
    "clf.score(X_scaled_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, using a more sophisticated model, such as a neural network, did not improve our prediction score compared to a simple logistic regression model. This outcome illustrates the principle that it is often better to start with simple models and only increase complexity when necessary. Simple models are easier to interpret, require less computational power, and are less prone to overfitting, especially when the dataset is small or not highly complex. Increasing model complexity should be considered when there is clear evidence that a more complex model can capture underlying patterns that a simpler model cannot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session4_practical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
