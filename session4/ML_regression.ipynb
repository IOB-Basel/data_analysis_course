{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, preprocessing\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Iris dataset\n",
    "\n",
    "The [Iris flower dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) is one of the \"toy datasets\" available in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_my_data(\n",
    "        iris,\n",
    "        *,\n",
    "        target_name: str = \"petal length (cm)\",\n",
    "        feature_name: Optional[str] = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    # Extract target (y) and features (x)\n",
    "    y_mask = np.array([n == target_name for n in iris.feature_names])\n",
    "    iris_y = iris.data[:, y_mask].ravel()  # ravel: flatten the array to 1-dimension\n",
    "\n",
    "    if feature_name is None:\n",
    "        iris_X = iris.data[:, ~y_mask]\n",
    "        feature_names = np.array(iris.feature_names)[~y_mask]\n",
    "    else:\n",
    "        x_mask = np.array([n == feature_name for n in iris.feature_names])\n",
    "        iris_X = iris.data[:, x_mask]\n",
    "        feature_names = np.array(iris.feature_names)[x_mask]\n",
    "\n",
    "    return iris_X, iris_y, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import and display the dataset and its description like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression: solve using ordinary least squares (OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object (model)\n",
    "regr = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple regression\n",
    "\n",
    "Petal length ~ Petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target (y) and features (x)\n",
    "target_name = \"petal length (cm)\"\n",
    "iris_X, iris_y, iris_features = load_my_data(\n",
    "    iris,\n",
    "    target_name=target_name,\n",
    "    feature_name=\"petal width (cm)\"\n",
    ")\n",
    "\n",
    "# Train the model using the data\n",
    "regr.fit(iris_X, iris_y)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "iris_y_pred = regr.predict(iris_X)\n",
    "\n",
    "print(f\"Coefficients:\")\n",
    "for name, coef in zip(iris_features, regr.coef_):\n",
    "    print(f\"{name}: {coef:.4f}\")\n",
    "print(f\"Intercept: {regr.intercept_:.4f}\")\n",
    "# Mean squared error\n",
    "print(f\"Mean squared error: {mean_squared_error(iris_y, iris_y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "df_plt = pd.DataFrame(\n",
    "    {\n",
    "        \"x\": iris_X.ravel(),\n",
    "        \"y\": iris_y,\n",
    "        \"y_pred\": iris_y_pred,\n",
    "    }\n",
    ")\n",
    "plt.scatter(df_plt[\"x\"], df_plt[\"y\"], color=\"black\")\n",
    "plt.plot(df_plt[\"x\"], df_plt[\"y_pred\"], \"b\")\n",
    "plt.xlabel(iris_features[0])\n",
    "plt.ylabel(target_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple regression\n",
    "\n",
    "Petal length ~ Petal width + Sepal length + Sepal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target (y) and features (x)\n",
    "target_name = \"petal length (cm)\"\n",
    "iris_X, iris_y, iris_features = load_my_data(\n",
    "    iris,\n",
    "    target_name=target_name,\n",
    ")\n",
    "iris_y.shape, iris_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the data\n",
    "regr.fit(iris_X, iris_y)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "iris_y_pred = regr.predict(iris_X)\n",
    "\n",
    "# Access coefficients\n",
    "print(f\"Coefficients: \\n\")\n",
    "for name, coef in zip(iris_features, regr.coef_):\n",
    "    print(f\"{name}: {coef:.4f}\")\n",
    "print(f\"Intercept: {regr.intercept_:.4f}\")\n",
    "# Mean squared error\n",
    "print(\"\\n\")\n",
    "print(f\"Mean squared error: {mean_squared_error(iris_y, iris_y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 4), sharey=True)\n",
    "\n",
    "# Plot data on each subplot\n",
    "for i, n in enumerate(iris_features):\n",
    "    axs[i].scatter(iris_X[:, i], iris_y, color=\"black\")\n",
    "    axs[i].set_xlabel(iris_features[i])\n",
    "axs[0].set_ylabel(target_name)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plt_loss(loss, xmax, xlab=\"iteration\", val_loss=None):\n",
    "    clear_output(wait=True)\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    ax.plot(loss, \"bo\")\n",
    "    if val_loss is not None:\n",
    "        ax.plot(val_loss, \"ro\", alpha=0.5)\n",
    "    ax.set_xlim(-0.1, xmax)\n",
    "    ax.set_xlabel(xlab)\n",
    "    ax.set_ylabel(\"loss\")\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def _plt_loss_params(df_data, loss, xmax):\n",
    "    clear_output(wait=True)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    ax[0].plot(loss, \"bo\")\n",
    "    ax[0].set_xlim(-0.1, xmax)\n",
    "    ax[0].set_xlabel(\"iteration\")\n",
    "    ax[0].set_ylabel(\"loss\")\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ax[1].plot(df_data[\"x\"], df_data[\"y\"], \"ko\")\n",
    "    ax[1].plot(df_data[\"x\"], df_data[\"y_pred\"], \"b\")\n",
    "    ax[1].set_xlabel(\"feature\")\n",
    "    ax[1].set_ylabel(\"target\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def gradient_descent_regression(\n",
    "    features: np.ndarray,\n",
    "    target: np.ndarray,\n",
    "    *,\n",
    "    learning_rate: float = 0.01,\n",
    "    n_iter: int = 100,\n",
    "    seed: int = 42,\n",
    "    plot: bool = True,\n",
    ") -> Tuple[np.ndarray, float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Find optimal weights and bias for linear regression task\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : numpy array of shape (n, p)\n",
    "    target : numpy array of shape (n,)\n",
    "    learning_rate : learning rate to control the update step size\n",
    "    n_iter : number of iterations\n",
    "    seed : seed for reproducibility\n",
    "    plot : plot gradient descent progress \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple containing the learned weights, bias and loss across iterations\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    # Initialize weights and bias with random values\n",
    "    n, p = features.shape\n",
    "    weights = np.random.rand(p)\n",
    "    bias = np.random.rand(1)\n",
    "\n",
    "    loss = np.zeros(n_iter)\n",
    "    if p == 1 and plot:\n",
    "        df_data = pd.DataFrame(\n",
    "            {\n",
    "                \"x\": features.ravel(),\n",
    "                \"y\": target,\n",
    "                \"y_pred\": np.zeros(target.shape[0]),\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Loop for the number of iterations\n",
    "    for i in range(n_iter):\n",
    "        # Predict target values using current weights and bias\n",
    "        predicted = np.dot(features, weights) + bias\n",
    "\n",
    "        # Calculate the error (to be used in the gradient, see below)\n",
    "        error = target - predicted\n",
    "        loss[i] = np.mean(error ** 2)\n",
    "\n",
    "        if plot:\n",
    "            # Update plot dynamically\n",
    "            if p == 1:\n",
    "                df_data[\"y_pred\"] = predicted\n",
    "                _plt_loss_params(df_data, loss[:i], n_iter)\n",
    "            else:\n",
    "                _plt_loss(loss[:i], n_iter)\n",
    "\n",
    "        # Calculate partial derivatives for weights and bias\n",
    "        weights_gradient = -2/n * np.dot(features.T, error)\n",
    "        bias_gradient = -2/n * np.sum(error)\n",
    "\n",
    "        # Update weights and bias using learning rate\n",
    "        weights -= learning_rate * weights_gradient\n",
    "        bias -= learning_rate * bias_gradient\n",
    "\n",
    "    print(\"Learned weights: \", weights)\n",
    "    print(\"Learned bias\", bias)\n",
    "    print(\"Loss:\", loss[-1])\n",
    "    return weights, bias, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target (y) and features (x)\n",
    "target_name = \"petal length (cm)\"\n",
    "iris_X, iris_y, iris_features = load_my_data(\n",
    "    iris,\n",
    "    target_name=target_name,\n",
    "    feature_name=\"petal width (cm)\"\n",
    ")\n",
    "\n",
    "weights, bias, loss = gradient_descent_regression(iris_X, iris_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current loss exceeds that achieved with OLS. Therefore, let's experiment with a higher learning rate to accelerate convergence. I also change the seed to modify the initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias, loss = gradient_descent_regression(iris_X, iris_y, seed=10, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results using ordinary least squares\n",
    "\n",
    "- petal width (cm): 2.2299\n",
    "- Intercept: 1.0836\n",
    "- Mean squared error: 0.2256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target (y) and features (x)\n",
    "target_name = \"petal length (cm)\"\n",
    "iris_X, iris_y, iris_features = load_my_data(\n",
    "    iris,\n",
    "    target_name=target_name,\n",
    ")\n",
    "\n",
    "weights, bias, loss = gradient_descent_regression(iris_X, iris_y, seed=10, learning_rate=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results using ordinary  least squares:\n",
    "- sepal length (cm): 0.7291\n",
    "- sepal width (cm): -0.6460\n",
    "- petal width (cm): 1.4468\n",
    "- Intercept: -0.2627\n",
    "- Mean squared error: 0.0990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: increase number of iterations\n",
    "weights, bias, loss = gradient_descent_regression(iris_X, iris_y, seed=10, learning_rate=0.02, plot=False, n_iter=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: standardize features\n",
    "scaler = preprocessing.StandardScaler().fit(iris_X)\n",
    "iris_X_scaled = scaler.transform(iris_X)\n",
    "weights, bias, loss = gradient_descent_regression(iris_X_scaled, iris_y, seed=10, learning_rate=0.02, plot=False, n_iter=2000)\n",
    "\n",
    "# Transform coefficients back to original scale\n",
    "weights_orig = weights / scaler.scale_\n",
    "bias_orig = bias - np.dot(scaler.mean_, weights / scaler.scale_)\n",
    "\n",
    "print(\"Learned weights: \", weights_orig)\n",
    "print(\"Learned bias\", bias_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that:\n",
    "- The initial weights chosen for the gradient descent algorithm play a role in determining the eventual solution. We can change the initialization by modifying the seed\n",
    "- The learning rate is pivotal in achieving convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_regression(\n",
    "    features: np.ndarray,\n",
    "    target: np.ndarray,\n",
    "    *,\n",
    "    learning_rate: float = 0.01,\n",
    "    epochs: int = 1000,\n",
    "    batch_size: int = 30,\n",
    "    seed: int = 42,\n",
    "    plot: bool = True,\n",
    ") -> Tuple[np.ndarray, float, float]:\n",
    "    \"\"\"\n",
    "    Perform Stochastic Gradient Descent with minibatches to fit a linear model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    features: input feature matrix (shape: [num_samples, num_features])\n",
    "    target: target values (shape: [num_samples])\n",
    "    learning_rate: learning rate for gradient descent\n",
    "    epochs: number of epochs for training\n",
    "    batch_size: size of the minibatches\n",
    "    seed : seed for reproducibility\n",
    "    plot : plot gradient descent progress \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple containing the learned weights (shape: [num_features]), bias and loss\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    # Initialize weights and bias with random values\n",
    "    n, p = features.shape\n",
    "    weights = np.random.rand(p)\n",
    "    bias = np.random.rand(1)\n",
    "\n",
    "    loss = []\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle the data\n",
    "        permutation = np.random.permutation(n)\n",
    "        X_shuffled = features[permutation]\n",
    "        y_shuffled = target[permutation]\n",
    "\n",
    "        # Create minibatches\n",
    "        for i in range(0, n, batch_size):\n",
    "            X_batch = X_shuffled[i:i + batch_size]\n",
    "            y_batch = y_shuffled[i:i + batch_size]\n",
    "\n",
    "            # Compute predictions\n",
    "            # Predict target values using current weights and bias\n",
    "            y_pred = np.dot(X_batch, weights) + bias\n",
    "\n",
    "            # Compute gradients\n",
    "            error = y_batch - y_pred\n",
    "            weights_gradient = -(2 / batch_size) * np.dot(X_batch.T, error)\n",
    "            bias_gradient = -(2 / batch_size) * np.sum(error)\n",
    "\n",
    "            # Update parameters\n",
    "            weights -= learning_rate * weights_gradient\n",
    "            bias -= learning_rate * bias_gradient\n",
    "\n",
    "        # Compute and record the loss for this epoch\n",
    "        y_pred_full = np.dot(features, weights) + bias\n",
    "        loss.append(np.mean((target - y_pred_full)**2))\n",
    "\n",
    "        # Print the loss every 100 epochs        \n",
    "        if epoch % 100 == 0:\n",
    "            if plot:\n",
    "                # Update plot dynamically\n",
    "                _plt_loss(loss, epochs, \"epoch\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss[-1]}\")\n",
    "\n",
    "    print(\"Learned weights: \", weights)\n",
    "    print(\"Learned bias\", bias)\n",
    "    print(\"Loss:\", loss[-1])\n",
    "    return weights, bias, loss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target (y) and features (x)\n",
    "target_name = \"petal length (cm)\"\n",
    "iris_X, iris_y, iris_features = load_my_data(\n",
    "    iris,\n",
    "    target_name=target_name,\n",
    ")\n",
    "\n",
    "weights, bias, loss = sgd_regression(iris_X, iris_y, learning_rate=0.001, batch_size=1, epochs=1010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn implements this algorithm cleverly. However, even for this straightforward example, optimizing the learning rate hyperparameters requires careful tuning to enhance performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SGDRegressor\n",
    "sgd_regr = SGDRegressor(loss=\"squared_error\", penalty=None, max_iter=1010, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = preprocessing.StandardScaler()\n",
    "iris_X_scaled = scaler.fit_transform(iris_X)\n",
    "\n",
    "# Fit the model\n",
    "sgd_regr.fit(iris_X_scaled, iris_y)\n",
    "\n",
    "iris_y_pred = sgd_regr.predict(iris_X_scaled)\n",
    "# Access coefficients\n",
    "print(f\"Coefficients: \\n\")\n",
    "for name, coef in zip(iris_features, sgd_regr.coef_):\n",
    "    print(f\"{name}: {coef:.4f}\")\n",
    "print(f\"Intercept: {sgd_regr.intercept_[0]:.4f}\")\n",
    "# Mean squared error\n",
    "print(\"\\n\")\n",
    "print(f\"Mean squared error: {mean_squared_error(iris_y, iris_y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training / test splits\n",
    "\n",
    "We will employ training/test splits to demonstrate how to effectively fine-tune the learning rate in our implementation of SGD and look at some \"real\" learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_splits(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_val: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    *,\n",
    "    learning_rate: float = 0.01,\n",
    "    epochs: int = 1000,\n",
    "    batch_size: int = 30,\n",
    "    seed: int = 42,\n",
    "    plot: bool = True,\n",
    ") -> Tuple[np.ndarray, float, float, float]:\n",
    "    \"\"\"\n",
    "    Perform Stochastic Gradient Descent with minibatches to fit a linear model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : input feature matrix for training (shape: [num_train_samples, num_features])\n",
    "    y_train : target values for training (shape: [num_train_samples])\n",
    "    X_val : input feature matrix for validation (shape: [num_val_samples, num_features])\n",
    "    y_val : target values for validation (shape: [num_val_samples])\n",
    "    learning_rate: learning rate for gradient descent\n",
    "    epochs: number of epochs for training\n",
    "    batch_size: size of the minibatches\n",
    "    seed : seed for reproducibility\n",
    "    plot : plot gradient descent progress \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple containing the learned weights (shape: [num_features]), bias, training and validation losses\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    # Initialize weights and bias with random values\n",
    "    n_train_samples, p = X_train.shape\n",
    "    weights = np.random.rand(p)\n",
    "    bias = np.random.rand(1)\n",
    "    \n",
    "    train_loss = np.zeros(epochs)\n",
    "    val_loss = np.zeros(epochs)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle the training data\n",
    "        permutation = np.random.permutation(n_train_samples)\n",
    "        X_train_shuffled = X_train[permutation]\n",
    "        y_train_shuffled = y_train[permutation]\n",
    "        \n",
    "        # Create minibatches\n",
    "        for i in range(0, n_train_samples, batch_size):\n",
    "            X_batch = X_train_shuffled[i:i + batch_size]\n",
    "            y_batch = y_train_shuffled[i:i + batch_size]\n",
    "            \n",
    "            # Compute predictions\n",
    "            y_pred = np.dot(X_batch, weights) + bias\n",
    "            \n",
    "            # Compute gradients\n",
    "            error = y_batch - y_pred\n",
    "            weights_gradient = -(2 / batch_size) * np.dot(X_batch.T, error)\n",
    "            bias_gradient = -(2 / batch_size) * np.sum(error)\n",
    "            \n",
    "            # Update parameters\n",
    "            weights -= learning_rate * weights_gradient\n",
    "            bias -= learning_rate * bias_gradient\n",
    "        \n",
    "        # Compute and record the training loss for this epoch\n",
    "        y_train_pred_full = np.dot(X_train, weights) + bias\n",
    "        train_loss[epoch] = np.mean((y_train - y_train_pred_full)**2)\n",
    "        \n",
    "        # Compute and record the validation loss for this epoch\n",
    "        y_val_pred_full = np.dot(X_val, weights) + bias\n",
    "        val_loss[epoch] = np.mean((y_val - y_val_pred_full)**2)\n",
    "        \n",
    "        # Print the loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            \n",
    "            if plot:\n",
    "                # Update plot dynamically\n",
    "                _plt_loss(train_loss[:epoch], epochs, \"epoch\", val_loss=val_loss[:epoch])\n",
    "            else:\n",
    "                print(f\"Epoch {epoch}, Training Loss: {train_loss[epoch]}, Validation Loss: {val_loss[epoch]}\")\n",
    "\n",
    "    print(\"Learned weights: \", weights)\n",
    "    print(\"Learned bias\", bias)\n",
    "    print(\"Training loss:\", train_loss[-1])\n",
    "    print(\"Validation loss:\", val_loss[-1])\n",
    "\n",
    "    return weights, bias, train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target (y) and features (x)\n",
    "target_name = \"petal length (cm)\"\n",
    "iris_X, iris_y, iris_features = load_my_data(\n",
    "    iris,\n",
    "    target_name=target_name,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris_X,\n",
    "    iris_y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.33,\n",
    "    random_state=42,\n",
    ")\n",
    "weights, bias, train_loss, val_loss = sgd_splits(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    epochs=1010,\n",
    "    batch_size=1,\n",
    "    learning_rate=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias, train_loss, val_loss_01 = sgd_splits(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    epochs=1010,\n",
    "    batch_size=1,\n",
    "    learning_rate=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias, train_loss, val_loss_0001 = sgd_splits(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    epochs=1010,\n",
    "    batch_size=1,\n",
    "    learning_rate=0.0001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(val_loss.shape[0]), val_loss_01, label=\"0.01\")\n",
    "plt.scatter(np.arange(val_loss.shape[0]), val_loss, label=\"0.001\")\n",
    "plt.scatter(np.arange(val_loss.shape[0]), val_loss_0001, label=\"0.0001\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session4_theory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
